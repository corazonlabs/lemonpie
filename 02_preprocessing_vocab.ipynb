{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab\n",
    "> Classes and functions to create vocabs from cleaned EHR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lemonpie.basics import *\n",
    "from lemonpie.preprocessing.clean import *\n",
    "from fastai.imports import *\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Embedding` and `nn.EmbeddingBag`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = nn.Embedding(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1523, -0.3658,  1.1193],\n",
       "         [ 0.0235, -0.1970,  0.2325],\n",
       "         [ 0.7203,  2.3237, -0.1296],\n",
       "         [ 0.5819, -0.2482, -0.3753],\n",
       "         [ 0.1162,  0.5788, -0.3371]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1(torch.LongTensor([[0,1,2,3,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding matrix is a lookup table\n",
    "1. `emb1` above has 5 rows, that is 5 elements\n",
    "2. but looking up an element, returns a vector for that element.\n",
    "\n",
    "Given this embedding matrix, looking up elements 1, 2, 4 will look like this .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0235, -0.1970,  0.2325],\n",
       "         [ 0.7203,  2.3237, -0.1296],\n",
       "         [ 0.1162,  0.5788, -0.3371]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch of inputs is also possible (in this case a batch of 2, each with 3 elements being looked up) \n",
    "- Note that inputs (# of elements being looked up) in a batch have to be of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4],[0,3,2]])\n",
    "# input = torch.LongTensor([[1,2,4],[0,3,2,1]]) # this will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0235, -0.1970,  0.2325],\n",
       "         [ 0.7203,  2.3237, -0.1296],\n",
       "         [ 0.1162,  0.5788, -0.3371]],\n",
       "\n",
       "        [[ 0.1523, -0.3658,  1.1193],\n",
       "         [ 0.5819, -0.2482, -0.3753],\n",
       "         [ 0.7203,  2.3237, -0.1296]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nn.EmbeddingBag`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embg1 = nn.EmbeddingBag(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly the same input as in case of `nn.Embedding` above (batch of 2)\n",
    "- but the result will be averaged across the 3 elements in a batch\n",
    "- resulting in an output of 2 vectors not 6 like above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4],[0,3,2]]) # exactly same as above, but o/p is avg'd now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3981, -0.1970, -0.5963],\n",
       "        [ 0.5032,  0.6250,  0.0026]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embg1(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this is to send in `offsets` rather than separating the inputs into 2 (or x number of) lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([1,2,4,0,3,2]) #same as above - 2 of same length 3\n",
    "offsets = torch.LongTensor([0,3]) # output will be avg'd by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3981, -0.1970, -0.5963],\n",
       "        [ 0.5032,  0.6250,  0.0026]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embg1(input, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([1,2,4,2,0,3,3,2]) #same as above - batch of 2 inputs, but of length 4 each\n",
    "offsets = torch.LongTensor([0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4415, -0.0460, -0.8777],\n",
       "        [ 0.3911,  0.8386,  0.4091]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embg1(input, offsets) #avg'd 2 outputs one for each input batch i.e. avg'd across 4 in each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Sizes**\n",
    "\n",
    "Offsets allow us to have input batches of different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([1,2,4,2,0,3,3,2]) #same input as above but .. \n",
    "offsets = torch.LongTensor([0,3,5]) #this indicates - 3 batches of different lengths (0,1,2)(3,4)(5,6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3981, -0.1970, -0.5963],\n",
       "        [ 0.7276,  0.1977, -0.8106],\n",
       "        [ 0.2269,  1.1220,  0.5119]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embg1(input, offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application to EHR Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `itoc`, `ctoi`, `ctod`, `numericalize`, `textify`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These have the same meaning as in fastai v1\n",
    "- `itoc` = index to code\n",
    "- `ctoi` = the reverse of `itoc`\n",
    "- `ctod` = is a new addition - to show descriptions in case descriptions exist in some type of EHR codes\n",
    "\n",
    "\n",
    "- `numericalize()` = returns numericalized ids (`ctoi`) for a set of codes\n",
    "- `textify()` = reverse of `numericalize()` - returns the codes (`itoc`) for a set of numericalized ids and if descriptions exist return them too (`ctod`)\n",
    "\n",
    "I tried to extend fastai vocabs, but found it easier to write from scratch, given EHR data is quite unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_raw_ehrdata(PATH_1K, today=SYNTHEA_DATAGEN_DATES['1K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dfs = load_ehr_vocabcodes(PATH_1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_codes, obs_codes, alg_codes, crpl_codes, med_codes, img_codes, proc_codes, cnd_codes, immn_codes = code_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following convenience function returns 2 results in order to make the given input a multiple of 8, to make it Tensor Core friendly for mixed precision training.\n",
    "1. `pad` - the extra padding needed to round up to the next multiple of 8.\n",
    "    - will be used during vocab creation to increase the vocab size **(round up)** with dummy padding elements.\n",
    "2. `opt` - for optimal - the number needed to round up or round down to the nearest multiple of 8 with a preference for rounding down.\n",
    "    - will be used when calculating the width for an embedding matrix based on a vocab.\n",
    "    - rounding up or down can be used to figure out **nearest** multiple of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def multiple_of_8(orig):\n",
    "    '''Return extra padding or optimization needed to make multiple of 8 for AMP.'''\n",
    "    mod = orig % 8\n",
    "    if mod == 0:\n",
    "        pad = opt = 0\n",
    "    else:\n",
    "        pad = 8 - mod\n",
    "        opt = -mod if mod <= 4 else (8-mod)\n",
    "\n",
    "    return pad, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number -- (pad, optimum)\n",
      "16 -- (0, 0)\n",
      "17 -- (7, -1)\n",
      "18 -- (6, -2)\n",
      "19 -- (5, -3)\n",
      "20 -- (4, -4)\n",
      "21 -- (3, 3)\n",
      "22 -- (2, 2)\n",
      "23 -- (1, 1)\n",
      "24 -- (0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(f'number -- (pad, optimum)')\n",
    "for i in range(16, 25):\n",
    "    print(f'{i} -- {multiple_of_8(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EhrVocab():\n",
    "    '''Vocab class for most EHR datatypes'''\n",
    "    def __init__(self, itoc, ctoi, ctod=None):\n",
    "        self.itoc = itoc\n",
    "        self.ctoi = ctoi\n",
    "        if ctod is not None: self.ctod = ctod \n",
    "        self.vocab_size = len(self.itoc)\n",
    "        \n",
    "    @classmethod\n",
    "    def create(cls, codes_df):\n",
    "        '''Create vocab object (itoc, ctoi and maybe ctod) from the codes df'''\n",
    "        desc_exists = 'desc' in codes_df.columns\n",
    "        codes_df = codes_df.astype({'code':'str'})\n",
    "        itoc = list(codes_df.code.unique())  #old --> list(set(codes_df.code))\n",
    "        itoc.insert(0,'xxnone')\n",
    "        itoc.insert(1,'xxunk')\n",
    "        \n",
    "        orig_len = len(itoc)\n",
    "        amp_pad_sz, _ = multiple_of_8(orig_len)\n",
    "        itoc.extend(['xxamp' for _ in range(amp_pad_sz)])\n",
    "        \n",
    "        ctoi = {code: i for i, code in enumerate(itoc)}\n",
    "        \n",
    "        if desc_exists:\n",
    "            codes_df.set_index('code', inplace=True)\n",
    "            ctod = {}\n",
    "            ctod[itoc[0]] = \"Nothing recorded\"\n",
    "            ctod[itoc[1]] = \"Unknown\"\n",
    "            for code in itoc[2:orig_len]: \n",
    "                ctod[code] = set(codes_df.loc[code].desc)\n",
    "            for code in itoc[orig_len:]:\n",
    "                ctod[code] = \"Padding for AMP\"\n",
    "        \n",
    "        return cls(itoc, ctoi, ctod) if desc_exists else cls(itoc, ctoi)\n",
    "    \n",
    "    def get_emb_dims(self, αd=0.5736):\n",
    "        '''Get embedding dimensions'''\n",
    "        width = round(6 * αd * (self.vocab_size**0.25))\n",
    "        _, amp_optimum = multiple_of_8(width)\n",
    "        width += amp_optimum\n",
    "        return self.vocab_size, width\n",
    "    \n",
    "    def numericalize(self, codes, log_excep=LOG_NUMERICALIZE_EXCEP, log_dir='default_log_store'):\n",
    "        '''Lookup and return indices for codes'''\n",
    "        \n",
    "        if log_excep:\n",
    "            today = date.today().strftime(\"%Y-%m-%d\")\n",
    "            log_dir = LOG_STORE if log_dir=='default_log_store' else log_dir\n",
    "            if not os.path.isdir(log_dir): os.mkdir(log_dir)\n",
    "            logfile = f'{log_dir}/{today}_numericalize_exceptions.log'\n",
    "                    \n",
    "        res = []\n",
    "        try:\n",
    "            res = [self.ctoi[str(code)] for code in codes] #no big performance benefit\n",
    "        except KeyError:\n",
    "            for code in codes:\n",
    "                try:\n",
    "                    res.append(self.ctoi[str(code)])\n",
    "                except KeyError:\n",
    "                    res.append(self.ctoi['xxunk'])\n",
    "                    if log_excep:\n",
    "                        with open(logfile, 'a') as log:\n",
    "                            log.write(f'\\ncode: {code}')                      \n",
    "                    \n",
    "        return res\n",
    "    \n",
    "    def textify(self, indxs):\n",
    "        '''Lookup and return descriptions for codes'''\n",
    "        if hasattr(self, 'ctod'):\n",
    "            res = [ (self.itoc[i], self.ctod[self.itoc[i]]) for i in indxs ]\n",
    "        else:\n",
    "            res = [ (self.itoc[i]) for i in indxs ]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"EhrVocab\" class=\"doc_header\"><code>class</code> <code>EhrVocab</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>EhrVocab</code>(**`itoc`**, **`ctoi`**, **`ctod`**=*`None`*)\n",
       "\n",
       "Vocab class for most EHR datatypes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocab.create\" class=\"doc_header\"><code>EhrVocab.create</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocab.create</code>(**`codes_df`**)\n",
       "\n",
       "Create vocab object (itoc, ctoi and maybe ctod) from the codes df"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocab.numericalize\" class=\"doc_header\"><code>EhrVocab.numericalize</code><a href=\"__main__.py#L44\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocab.numericalize</code>(**`codes`**, **`log_excep`**=*`True`*, **`log_dir`**=*`'default_log_store'`*)\n",
       "\n",
       "Lookup and return indices for codes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab.numericalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocab.textify\" class=\"doc_header\"><code>EhrVocab.textify</code><a href=\"__main__.py#L68\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocab.textify</code>(**`indxs`**)\n",
       "\n",
       "Lookup and return descriptions for codes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab.textify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocab.get_emb_dims\" class=\"doc_header\"><code>EhrVocab.get_emb_dims</code><a href=\"__main__.py#L37\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocab.get_emb_dims</code>(**`αd`**=*`0.5736`*)\n",
       "\n",
       "Get embedding dimensions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab.get_emb_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ObsVocab (EhrVocab):\n",
    "    '''Special Vocab class for Observation codes'''\n",
    "    def __init__(self, vocab_df):\n",
    "        self.vocab_df = vocab_df\n",
    "        self.vocab_size = len(vocab_df)\n",
    "    \n",
    "    def numericalize(self, codes, log_excep=LOG_NUMERICALIZE_EXCEP, log_dir='default_log_store'):\n",
    "        '''Numericalize observation codes (return indices for codes)'''\n",
    "        \n",
    "        if log_excep:\n",
    "            today = date.today().strftime(\"%Y-%m-%d\")\n",
    "            log_dir = LOG_STORE if log_dir=='default_log_store' else log_dir\n",
    "            if not os.path.isdir(log_dir): os.mkdir(log_dir)\n",
    "            logfile = f'{log_dir}/{today}_numericalize_exceptions.log'\n",
    "        \n",
    "        indxs = []\n",
    "        for code in codes:\n",
    "            if code in ['xxnone','xxunk']: indxs.extend(self.vocab_df[(self.vocab_df['code'] == code)].index.tolist())\n",
    "            else: \n",
    "                c,v,u,t = code.split('||')\n",
    "                if t == 'numeric':\n",
    "                    filt_df = self.vocab_df[(self.vocab_df['code'] == c) & (self.vocab_df['units'] == u) & (self.vocab_df['type'] == t)]\n",
    "                    res = filt_df.iloc[(filt_df.value - float(v)).abs().argsort()[:1]].index.tolist()\n",
    "                else:\n",
    "                    res = self.vocab_df[(self.vocab_df['code'] == c) & (self.vocab_df['value'] == v) & \\\n",
    "                                               (self.vocab_df['units'] == u) & (self.vocab_df['type'] == t)].index.tolist()\n",
    "                if len(res) == 0: \n",
    "                    indxs.extend(self.vocab_df[(self.vocab_df['code'] == 'xxunk')].index.tolist())\n",
    "                    if log_excep:\n",
    "                        with open(logfile, 'a') as log:\n",
    "                            log.write(f'\\ncode in ObsVocab: {code}')                    \n",
    "                else            : indxs.extend(res)\n",
    "                    \n",
    "        assert len(codes) == len(indxs), \"Possible bug, not all codes being numericalized\"\n",
    "        return indxs\n",
    "    \n",
    "    def textify(self, indxs):\n",
    "        '''Textify observation codes (returns codes and descriptions)'''\n",
    "        txts = []\n",
    "        for i in indxs:\n",
    "            c,d,v,u,t = self.vocab_df.iloc[i]\n",
    "            if i == 0: txts.append((c, d))\n",
    "            else:      txts.append((f'{c}||{v}||{u}||{t}', d))\n",
    "        assert len(indxs) == len(txts), \"Possible bug, not all indxs being textified\"\n",
    "        return txts\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, obs_codes, num_buckets=5):\n",
    "        '''Create vocab object from observation codes'''\n",
    "        numerics = pd.DataFrame(obs_codes.loc[obs_codes['type'] == 'numeric',:])\n",
    "        texts = pd.DataFrame(obs_codes.loc[obs_codes['type'] == 'text',:])\n",
    "        numerics = numerics.astype({'value':'float'}, copy=False)\n",
    "        vocab_rows = []\n",
    "\n",
    "        for code in numerics.orig_code.unique():\n",
    "            this_code = numerics.loc[numerics['orig_code'] == code]\n",
    "            for unit in this_code.units.unique():\n",
    "                this_unit = this_code.loc[this_code['units'] == unit]\n",
    "                for val in np.linspace(this_unit.value.min(), this_unit.value.max(), num=num_buckets):\n",
    "                    vocab_rows.append([code,this_unit.desc.iloc[0],val,unit,'numeric'])\n",
    "\n",
    "        for code in texts.orig_code.unique():\n",
    "            this_code = texts.loc[texts['orig_code'] == code]\n",
    "            for unit in this_code.units.unique():\n",
    "                this_unit = this_code.loc[this_code['units'] == unit]\n",
    "                for val in this_unit.value.unique():\n",
    "                    vocab_rows.append([code,this_unit.desc.iloc[0],val,unit,'text'])\n",
    "                    \n",
    "        vocab_rows.insert(0, ['xxnone','Nothing recorded','xxnone','xxnone','xxnone'])\n",
    "        vocab_rows.insert(1, ['xxunk','Unknown','xxunk','xxunk','xxunk'])\n",
    "        amp_pad_sz, _ = multiple_of_8(len(vocab_rows)) \n",
    "        for _ in range(amp_pad_sz):\n",
    "            vocab_rows.append(['xxamp','Padding for AMP','xxamp','xxamp','xxamp'])\n",
    "        \n",
    "        obs_vocab = pd.DataFrame(data=vocab_rows, columns=['code','desc','value','units','type'])\n",
    "        assert obs_codes.orig_code.nunique() == obs_vocab.code.nunique() - 3, \"Possible bug, obs_code nuniques don't match\"\n",
    "        \n",
    "        return cls(obs_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"ObsVocab\" class=\"doc_header\"><code>class</code> <code>ObsVocab</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>ObsVocab</code>(**`vocab_df`**) :: [`EhrVocab`](/lemonpie/preprocessing_vocab.html#EhrVocab)\n",
       "\n",
       "Special Vocab class for Observation codes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ObsVocab, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ObsVocab.create\" class=\"doc_header\"><code>ObsVocab.create</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ObsVocab.create</code>(**`obs_codes`**, **`num_buckets`**=*`5`*)\n",
       "\n",
       "Create vocab object from observation codes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ObsVocab.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ObsVocab.numericalize\" class=\"doc_header\"><code>ObsVocab.numericalize</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ObsVocab.numericalize</code>(**`codes`**, **`log_excep`**=*`True`*, **`log_dir`**=*`'default_log_store'`*)\n",
       "\n",
       "Numericalize observation codes (return indices for codes)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ObsVocab.numericalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split incoming concated `code||value||units||type` string\n",
    "- get a result_df based on everything except value\n",
    "- then do an `argsort()` on the value column to determine closest value\n",
    " - based on example given in [pandas docs - cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html#building-criteria)\n",
    "   - **cookbook example that uses `loc` doesnt work, instead `iloc` [works](https://stackoverflow.com/questions/30112202/how-do-i-find-the-closest-values-in-a-pandas-series-to-an-input-number/53553226)**\n",
    " - `argsort()` - [Returns the indices that would sort this array](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort)\n",
    " - `[:1]` on that returns the one row with the closest match, index of that is what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ObsVocab.textify\" class=\"doc_header\"><code>ObsVocab.textify</code><a href=\"__main__.py#L38\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ObsVocab.textify</code>(**`indxs`**)\n",
       "\n",
       "Textify observation codes (returns codes and descriptions)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ObsVocab.textify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: During a pre-processing run, the default behavior is to log any numericalize errors that are encountered. This is controlled by the global-level variable `LOG_NUMERICALIZE_EXCEP` which is set to true by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: `create()` and `get_emb_dims()` methods have logic in them to create vocab lengths and widths respectively that are Tensor Core-friendly, (i.e. multiples of 8) to help with mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_code</th>\n",
       "      <th>desc</th>\n",
       "      <th>value</th>\n",
       "      <th>units</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8302-2</td>\n",
       "      <td>Body Height</td>\n",
       "      <td>169.6</td>\n",
       "      <td>cm</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72514-3</td>\n",
       "      <td>Pain severity - 0-10 verbal numeric rating [Sc...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{score}</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29463-7</td>\n",
       "      <td>Body Weight</td>\n",
       "      <td>63.8</td>\n",
       "      <td>kg</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39156-5</td>\n",
       "      <td>Body Mass Index</td>\n",
       "      <td>22.2</td>\n",
       "      <td>kg/m2</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59576-9</td>\n",
       "      <td>Body mass index (BMI) [Percentile] Per age and...</td>\n",
       "      <td>81.9</td>\n",
       "      <td>%</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     orig_code                                               desc  value  \\\n",
       "indx                                                                       \n",
       "0       8302-2                                        Body Height  169.6   \n",
       "1      72514-3  Pain severity - 0-10 verbal numeric rating [Sc...    4.0   \n",
       "2      29463-7                                        Body Weight   63.8   \n",
       "3      39156-5                                    Body Mass Index   22.2   \n",
       "4      59576-9  Body mass index (BMI) [Percentile] Per age and...   81.9   \n",
       "\n",
       "        units     type  \n",
       "indx                    \n",
       "0          cm  numeric  \n",
       "1     {score}  numeric  \n",
       "2          kg  numeric  \n",
       "3       kg/m2  numeric  \n",
       "4           %  numeric  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_vocab_obj = ObsVocab.create(obs_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 341, 16]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.numericalize(['8302-2||200.3||cm||numeric', \\\n",
    "                            '72514-3||4||{score}||numeric', '33756-8||21.7||mm||numeric','29463-7||181.8||kg||numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 9, 341, 16]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing unkown code\n",
    "obs_vocab_obj.numericalize(['blah-2||200.3||cm||numeric', \\\n",
    "                            '72514-3||4||{score}||numeric', '33756-8||21.7||mm||numeric','29463-7||181.8||kg||numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8302-2||160.29999999999998||cm||numeric', 'Body Height'),\n",
       " ('72514-3||2.5||{score}||numeric',\n",
       "  'Pain severity - 0-10 verbal numeric rating [Score] - Reported'),\n",
       " ('20565-8||26.75||mmol/L||numeric', 'Carbon Dioxide'),\n",
       " ('29463-7||98.09999999999998||kg||numeric', 'Body Weight')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.textify([5, 8, 200, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[522, 1, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.numericalize(['32465-7||Normal size prostate||{nominal}||text',\"80271-0||Positive Murphy's Sign||xxxnan||text\",\\\n",
    "                          'xxnone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('32465-7||Normal size prostate||{nominal}||text',\n",
       "  'Physical findings of Prostate'),\n",
       " ('32465-7||Prostate enlarged on PR||{nominal}||text',\n",
       "  'Physical findings of Prostate'),\n",
       " ('xxnone', 'Nothing recorded')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.textify([522, 523, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 467]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.numericalize(['xxnone','xxunk','72166-2||Never smoker||xxxnan||text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxnone', 'Nothing recorded'),\n",
       " ('xxunk||xxunk||xxunk||xxunk', 'Unknown'),\n",
       " ('8302-2||45.1||cm||numeric', 'Body Height'),\n",
       " ('8302-2||83.5||cm||numeric', 'Body Height'),\n",
       " ('72166-2||Never smoker||xxxnan||text', 'Tobacco smoking status NHIS'),\n",
       " ('88040-1||Improving (qualifier value)||xxxnan||text',\n",
       "  'Response to cancer treatment')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.textify([0, 1, 2, 3, 467, 497])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EhrVocabList:\n",
    "    '''Class to create and hold all vocab objects for an entire dataset'''\n",
    "    def __init__(self, demographics_vocabs, records_vocabs, age_mean, age_std, path):\n",
    "        self.demographics_vocabs, self.records_vocabs, self.path = demographics_vocabs, records_vocabs, path\n",
    "        self.age_mean, self.age_std = age_mean, age_std\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, path, num_buckets=5):\n",
    "        '''Read all code dfs from the dataset path and create all vocab objects'''\n",
    "        demographics_vocabs, records_vocabs = [], []\n",
    "        code_dfs = load_ehr_vocabcodes(path)\n",
    "        \n",
    "        def _get_demographics_codes(pt_codes):\n",
    "            code_dfs = []\n",
    "            code_dfs.extend([pd.DataFrame(range(1, 32, 1), columns=['code'])]) #31 days  \n",
    "            code_dfs.extend([pd.DataFrame(range(1, 13, 1), columns=['code'])]) #12 months \n",
    "            code_dfs.extend([pd.DataFrame(range(1900, pd.Timestamp.today().year + 1, 1), columns=['code'])]) #years 1900 to now\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.marital.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.race.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.ethnicity.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.gender.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.birthplace.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.city.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.state.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.zip.dropna().unique(), columns=['code'])])\n",
    "            age_mean, age_std = pt_codes.age_now_days.mean(), pt_codes.age_now_days.std()\n",
    "            return code_dfs, age_mean, age_std\n",
    "        \n",
    "        demographics_codes, age_mean, age_std = _get_demographics_codes(code_dfs[0])\n",
    "        demographics_vocabs.extend([EhrVocab.create(codes_df) for codes_df in demographics_codes])\n",
    "        records_vocabs.extend([ObsVocab.create(code_dfs[1], num_buckets)])\n",
    "        records_vocabs.extend([EhrVocab.create(codes_df) for codes_df in code_dfs[2:]])\n",
    "        return cls(demographics_vocabs, records_vocabs, age_mean, age_std, path)    \n",
    "    \n",
    "    def save(self):\n",
    "        '''Save vocablist (containing all vocab objects for the dataset)'''\n",
    "        pckl_dir = Path(f'{self.path}/processed')\n",
    "        pckl_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pckl_f = open(f'{pckl_dir}/vocabs.vocablist', 'wb')\n",
    "        pickle.dump(self, pckl_f)\n",
    "        pckl_f.close()\n",
    "        print(f'Saved vocab lists to {pckl_dir}')\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        '''Load previously created vocablist object (containing all vocab objects for the dataset)'''\n",
    "        infile = open(f'{path}/processed/vocabs.vocablist','rb')\n",
    "        ehrVocabList = pickle.load(infile)\n",
    "        infile.close()\n",
    "        return ehrVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"EhrVocabList\" class=\"doc_header\"><code>class</code> <code>EhrVocabList</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>EhrVocabList</code>(**`demographics_vocabs`**, **`records_vocabs`**, **`age_mean`**, **`age_std`**, **`path`**)\n",
       "\n",
       "Class to create and hold all vocab objects for an entire dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocabList, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocabList.create\" class=\"doc_header\"><code>EhrVocabList.create</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocabList.create</code>(**`path`**, **`num_buckets`**=*`5`*)\n",
       "\n",
       "Read all code dfs from the dataset path and create all vocab objects"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocabList.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocabList.save\" class=\"doc_header\"><code>EhrVocabList.save</code><a href=\"__main__.py#L36\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocabList.save</code>()\n",
       "\n",
       "Save vocablist (containing all vocab objects for the dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocabList.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocabList.load\" class=\"doc_header\"><code>EhrVocabList.load</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocabList.load</code>(**`path`**)\n",
       "\n",
       "Load previously created vocablist object (containing all vocab objects for the dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocabList.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list_1K = EhrVocabList.create(PATH_1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocab lists to /home/vinod/.lemonpie/datasets/synthea/1K/processed\n"
     ]
    }
   ],
   "source": [
    "vocab_list_1K.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_1K = EhrVocabList.load(PATH_1K)\n",
    "obs_vocab, alg_vocab, crpl_vocab, med_vocab, img_vocab, proc_vocab, cnd_vocab, imm_vocab = vl_1K.records_vocabs\n",
    "bday, bmonth, byear, marital, race, ethnicity, gender, birthplace, city, state, zipcode  = vl_1K.demographics_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`records_vocabs`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 67, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_vocab.numericalize(['xxnone','65200003','428191000124101'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 8]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vocab.numericalize(['xxnone',344001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([67], [67])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_vocab.numericalize(['65200003']), proc_vocab.numericalize([65200003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxnone', 'Nothing recorded'),\n",
       " ('xxunk', 'Unknown'),\n",
       " ('40983000', {'Arm'}),\n",
       " ('51299004', {'Clavicle'}),\n",
       " ('8205005', {'Wrist'}),\n",
       " ('72696002', {'Knee'})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vocab.textify([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 7, 9]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vocab.numericalize(['xxnone','xxunk', 51299004,51185008,12921003]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxnone', 'Nothing recorded'),\n",
       " ('xxunk||xxunk||xxunk||xxunk', 'Unknown'),\n",
       " ('8302-2||45.1||cm||numeric', 'Body Height'),\n",
       " ('8302-2||83.5||cm||numeric', 'Body Height'),\n",
       " ('8302-2||121.9||cm||numeric', 'Body Height'),\n",
       " ('8302-2||160.29999999999998||cm||numeric', 'Body Height')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.textify([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20565-8||26.75||mmol/L||numeric', 'Carbon Dioxide')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.textify([200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 1, 16]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.numericalize(['8302-2||200.3||cm||numeric', \\\n",
    "                            '72514-3||4||{score}||numeric', '10834-0||3.7||g/dL||numeric','29463-7||181.8||kg||numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8310-5||38.724999999999994||Cel||numeric', 'Body temperature'),\n",
       " ('2085-9||65.05||mg/dL||numeric', 'High Density Lipoprotein Cholesterol'),\n",
       " ('6248-9||56.175000000000004||kU/L||numeric', 'Soybean IgE Ab in Serum'),\n",
       " ('33914-3||120.875||mL/min/{1.73_m2}||numeric',\n",
       "  'Estimated Glomerular Filtration Rate')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.textify([50,150,250,300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxnone', 'Nothing recorded'),\n",
       " ('xxunk', 'Unknown'),\n",
       " ('313782||START', {'Acetaminophen 325 MG Oral Tablet'}),\n",
       " ('748856||START', {'Yaz 28 Day Pack'}),\n",
       " ('1534809||START',\n",
       "  {'168 HR Ethinyl Estradiol 0.00146 MG/HR / norelgestromin 0.00625 MG/HR Transdermal System'})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.textify([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxnone', 'xxunk', '313782||START', '748856||START', '1534809||START']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.itoc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1873983||STOP',\n",
       " '1734340||STOP',\n",
       " '313185||STOP',\n",
       " '1650142||STOP',\n",
       " 'xxamp',\n",
       " 'xxamp',\n",
       " 'xxamp',\n",
       " 'xxamp',\n",
       " 'xxamp',\n",
       " 'xxamp']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.itoc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 24, 1, 2, 66]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.numericalize(['xxnone', 'xxunk', '834061||START','282464||START', '313782||START', '749882||START'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.numericalize(['834061||START'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`demographics_vocabs`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 8)\n",
      "(16, 8)\n",
      "(128, 8)\n",
      "(8, 8)\n",
      "(8, 8)\n",
      "(8, 8)\n",
      "(8, 8)\n",
      "(248, 16)\n",
      "(208, 16)\n",
      "(8, 8)\n",
      "(184, 16)\n"
     ]
    }
   ],
   "source": [
    "for vocab in vl_1K.demographics_vocabs:\n",
    "    print(vocab.get_emb_dims())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 11, 32]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bday.numericalize(['xxnone','xxunk', 1,10,31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxnone', 'xxunk', '1', '10', '31']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bday.textify([0, 1, 2, 11, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmonth.textify([13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 49]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byear.numericalize(['1942',1947,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byear.numericalize([1948])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marital.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xxnone': 0, 'xxunk': 1, 'M': 2, 'xxxnan': 3, 'S': 4, 'xxamp': 7}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marital.ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxnone', 'xxunk', 'M', 'xxxnan', 'S', 'xxamp']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marital.textify([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxnone', 'xxunk', 'black', 'white', 'asian']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race.textify([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xxnone': 0,\n",
       " 'xxunk': 1,\n",
       " 'black': 2,\n",
       " 'white': 3,\n",
       " 'asian': 4,\n",
       " 'native': 5,\n",
       " 'other': 6,\n",
       " 'xxamp': 7}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race.ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16313.340455840456, 9600.296817631992)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_1K.age_mean, vl_1K.age_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All Embedding Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_all_emb_dims(EhrVocabList, αd=0.5736):\n",
    "    '''Get embedding dimensions for all vocab objects of the dataset'''\n",
    "    demographics_dims = [vocab.get_emb_dims(αd) for vocab in EhrVocabList.demographics_vocabs]\n",
    "    recs_dims          = [vocab.get_emb_dims(αd) for vocab in EhrVocabList.records_vocabs]\n",
    "    \n",
    "    demographics_dims_width = recs_dims_width = 0\n",
    "    for emb_dim in demographics_dims:\n",
    "        demographics_dims_width += emb_dim[1]\n",
    "    for emb_dim in recs_dims:\n",
    "        recs_dims_width += emb_dim[1]\n",
    "        \n",
    "    return demographics_dims, recs_dims, demographics_dims_width, recs_dims_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dims, recs_dims, demographics_dims_width, recs_dims_width = get_all_emb_dims(EhrVocabList.load(PATH_1K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40, 8),\n",
       " (16, 8),\n",
       " (128, 8),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (248, 16),\n",
       " (208, 16),\n",
       " (8, 8),\n",
       " (184, 16)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40, 8),\n",
       " (16, 8),\n",
       " (128, 8),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (248, 16),\n",
       " (208, 16),\n",
       " (8, 8),\n",
       " (184, 16)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(536, 16), (32, 8), (56, 8), (232, 16), (16, 8), (144, 8), (184, 16), (24, 8)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 88)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims_width, recs_dims_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 88)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims_width, recs_dims_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`αd` - a hyperparameter to control the width of the embedding matrices\n",
    "- higher values result in wider embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dims, recs_dims, demographics_dims_width, recs_dims_width = get_all_emb_dims(EhrVocabList.load(PATH_1K), αd=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40, 152),\n",
       " (16, 120),\n",
       " (128, 200),\n",
       " (8, 104),\n",
       " (8, 104),\n",
       " (8, 104),\n",
       " (8, 104),\n",
       " (248, 240),\n",
       " (208, 224),\n",
       " (8, 104),\n",
       " (184, 224)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(536, 288),\n",
       " (32, 144),\n",
       " (56, 160),\n",
       " (232, 232),\n",
       " (16, 120),\n",
       " (144, 208),\n",
       " (184, 224),\n",
       " (24, 136)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1680, 1512)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims_width, recs_dims_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_basics.ipynb.\n",
      "Converted 01_preprocessing_clean.ipynb.\n",
      "Converted 02_preprocessing_vocab.ipynb.\n",
      "Converted 03_preprocessing_transform.ipynb.\n",
      "Converted 04_data.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_learn.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 08_experiment.ipynb.\n",
      "Converted 998_coherent_clean.ipynb.\n",
      "Converted 999_amp_testing.ipynb.\n",
      "Converted 99_quick_walkthru.ipynb.\n",
      "Converted 99_running_exps.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('lemonpie')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
