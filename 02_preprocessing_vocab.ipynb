{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lemonade.preprocessing.clean import *\n",
    "from fastai.imports import *\n",
    "from fastai import *\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Embedding` and `nn.EmbeddingBag`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = nn.Embedding(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8246, -0.4822, -0.9968],\n",
       "         [-0.3546,  1.2317,  0.9929],\n",
       "         [ 0.6679, -0.1507, -2.3095],\n",
       "         [-1.6767, -0.4624, -0.6374],\n",
       "         [ 0.7486, -1.2633, -0.1052]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1(torch.LongTensor([[0,1,2,3,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding matrix is a lookup table\n",
    "1. `emb1` above has 5 rows, that is 5 elements\n",
    "2. but looking up an element, returns a vector for that element.\n",
    "\n",
    "Given this embedding matrix, looking up elements 1, 2, 4 will look like this .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3546,  1.2317,  0.9929],\n",
       "         [ 0.6679, -0.1507, -2.3095],\n",
       "         [ 0.7486, -1.2633, -0.1052]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch of inputs is also possible (in this case a batch of 2, each with 3 elements being looked up) \n",
    "- Note that inputs (# of elements being looked up) in a batch have to be of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4],[0,3,2]])\n",
    "# input = torch.LongTensor([[1,2,4],[0,3,2,1]]) # this will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3546,  1.2317,  0.9929],\n",
       "         [ 0.6679, -0.1507, -2.3095],\n",
       "         [ 0.7486, -1.2633, -0.1052]],\n",
       "\n",
       "        [[-0.8246, -0.4822, -0.9968],\n",
       "         [-1.6767, -0.4624, -0.6374],\n",
       "         [ 0.6679, -0.1507, -2.3095]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb1(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nn.EmbeddingBag`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embg1 = nn.EmbeddingBag(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly the same input as in case of `nn.Embedding` above (batch of 2)\n",
    "- but the result will be averaged across the 3 elements in a batch\n",
    "- resulting in an output of 2 vectors not 6 like above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4],[0,3,2]]) # exactly same as above, but o/p is avg'd now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2607, -0.2411, -0.1735],\n",
       "        [-0.4628,  0.1948,  0.2288]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embg1(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this is to send in `offsets` rather than separating the inputs into 2 (or x number of) lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([1,2,4,0,3,2]) #same as above - 2 of same length 3\n",
    "offsets = torch.LongTensor([0,3]) # output will be avg'd by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2607, -0.2411, -0.1735],\n",
       "        [-0.4628,  0.1948,  0.2288]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embg1(input, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([1,2,4,2,0,3,3,2]) #same as above - batch of 2 inputs, but of length 4 each\n",
    "offsets = torch.LongTensor([0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3061, -0.6667,  0.0523],\n",
       "        [-0.3239,  0.3875,  0.5752]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embg1(input, offsets) #avg'd 2 outputs one for each input batch i.e. avg'd across 4 in each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Sizes**\n",
    "\n",
    "Offsets allow us to have input batches of different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([1,2,4,2,0,3,3,2]) #same input as above but .. \n",
    "offsets = torch.LongTensor([0,3,5]) #this indicates - 3 batches of different lengths (0,1,2)(3,4)(5,6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2607, -0.2411, -0.1735],\n",
       "        [-0.7404, -0.1908, -0.4639],\n",
       "        [ 0.2092, -0.0039,  1.3196]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embg1(input, offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application to EHR Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `itoc`, `ctoi`, `ctod`, `numericalize`, `textify`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to extend fastai vocabs, but found it easier to write from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dfs = load_ehr_vocabcodes(PATH_1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_codes, obs_codes, alg_codes, crpl_codes, med_codes, img_codes, proc_codes, cnd_codes, immn_codes = code_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EhrVocab():\n",
    "    '''Vocab class for most EHR datatypes'''\n",
    "    def __init__(self, itoc, ctoi, ctod=None):\n",
    "        self.itoc = itoc\n",
    "        self.ctoi = ctoi\n",
    "        if ctod is not None: self.ctod = ctod \n",
    "        self.vocab_size = len(self.itoc)\n",
    "        \n",
    "    @classmethod\n",
    "    def create(cls, codes_df):\n",
    "        '''Create vocab object (itoc, ctoi and maybe ctod) from the codes df'''\n",
    "        desc_exists = 'desc' in codes_df.columns\n",
    "        codes_df = codes_df.astype({'code':'str'})\n",
    "        itoc = list(codes_df.code.unique())  #old --> list(set(codes_df.code))\n",
    "        itoc.insert(0,'xxnone')\n",
    "        itoc.insert(1,'xxunk')\n",
    "        \n",
    "        ctoi = {code: i for i, code in enumerate(itoc)}\n",
    "        \n",
    "        if desc_exists:\n",
    "            codes_df.set_index('code', inplace=True)\n",
    "            ctod = {}\n",
    "            ctod[itoc[0]] = \"Nothing recorded\"\n",
    "            ctod[itoc[1]] = \"Unknown\"\n",
    "            for code in itoc[2:]: \n",
    "                ctod[code] = set(codes_df.loc[code].desc)\n",
    "        \n",
    "        return cls(itoc, ctoi, ctod) if desc_exists else cls(itoc, ctoi)\n",
    "    \n",
    "    def get_emb_dims(self, αd=0.5736):\n",
    "        '''Get embedding dimensions'''\n",
    "        return self.vocab_size, round(6* αd * (self.vocab_size**0.25))\n",
    "    \n",
    "    def numericalize(self, codes, verbose=True):\n",
    "        '''Lookup and return indices for codes'''\n",
    "        today = date.today().strftime(\"%Y-%m-%d\")\n",
    "        logfile = f'./log/{today}_numericalize_exceptions.log'\n",
    "        \n",
    "        res = []\n",
    "        try:\n",
    "            res = [self.ctoi[str(code)] for code in codes] #no big performance benefit\n",
    "        except KeyError:\n",
    "            for code in codes:\n",
    "                try:\n",
    "                    res.append(self.ctoi[str(code)])\n",
    "                except KeyError:\n",
    "                    res.append(self.ctoi['xxunk'])\n",
    "                    if verbose:\n",
    "                        with open(logfile, 'a') as log:\n",
    "                            log.write(f'\\ncode: {code}')                      \n",
    "                    \n",
    "        return res\n",
    "    \n",
    "    def textify(self, indxs):\n",
    "        '''Lookup and return descriptions for codes'''\n",
    "        if hasattr(self, 'ctod'):\n",
    "            res = [ (self.itoc[i], self.ctod[self.itoc[i]]) for i in indxs ]\n",
    "        else:\n",
    "            res = [ (self.itoc[i]) for i in indxs ]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocab.create\" class=\"doc_header\"><code>EhrVocab.create</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocab.create</code>(**`codes_df`**)\n",
       "\n",
       "Create vocab object (itoc, ctoi and maybe ctod) from the codes df"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocab.numericalize\" class=\"doc_header\"><code>EhrVocab.numericalize</code><a href=\"__main__.py#L35\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocab.numericalize</code>(**`codes`**, **`verbose`**=*`True`*)\n",
       "\n",
       "Lookup and return indices for codes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab.numericalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocab.textify\" class=\"doc_header\"><code>EhrVocab.textify</code><a href=\"__main__.py#L55\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocab.textify</code>(**`indxs`**)\n",
       "\n",
       "Lookup and return descriptions for codes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab.textify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocab.get_emb_dims\" class=\"doc_header\"><code>EhrVocab.get_emb_dims</code><a href=\"__main__.py#L31\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocab.get_emb_dims</code>(**`αd`**=*`0.5736`*)\n",
       "\n",
       "Get embedding dimensions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocab.get_emb_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ObsVocab (EhrVocab):\n",
    "    '''Special Vocab class for Observation codes'''\n",
    "    def __init__(self, vocab_df):\n",
    "        self.vocab_df = vocab_df\n",
    "        self.vocab_size = len(vocab_df)\n",
    "    \n",
    "    def numericalize(self, codes, verbose=True, log_dir='./log'):\n",
    "        '''Numericalize observation codes (return indices for codes)'''\n",
    "        today = date.today().strftime(\"%Y-%m-%d\")\n",
    "        if verbose:\n",
    "            if not os.path.isdir(log_dir): os.mkdir(log_dir)\n",
    "            logfile = f'./log/{today}_numericalize_exceptions.log'\n",
    "        \n",
    "        indxs = []\n",
    "        for code in codes:\n",
    "            if code in ['xxnone','xxunk']: indxs.extend(self.vocab_df[(self.vocab_df['code'] == code)].index.tolist())\n",
    "            else: \n",
    "                c,v,u,t = code.split('||')\n",
    "                if t == 'numeric':\n",
    "                    filt_df = self.vocab_df[(self.vocab_df['code'] == c) & (self.vocab_df['units'] == u) & (self.vocab_df['type'] == t)]\n",
    "                    res = filt_df.iloc[(filt_df.value - float(v)).abs().argsort()[:1]].index.tolist()\n",
    "                else:\n",
    "                    res = self.vocab_df[(self.vocab_df['code'] == c) & (self.vocab_df['value'] == v) & \\\n",
    "                                               (self.vocab_df['units'] == u) & (self.vocab_df['type'] == t)].index.tolist()\n",
    "                if len(res) == 0: \n",
    "                    indxs.extend(self.vocab_df[(self.vocab_df['code'] == 'xxunk')].index.tolist())\n",
    "                    if verbose:\n",
    "                        with open(logfile, 'a') as log:\n",
    "                            log.write(f'\\ncode in ObsVocab: {code}')                    \n",
    "                else            : indxs.extend(res)\n",
    "        assert len(codes) == len(indxs), \"Possible bug, not all codes being numericalized\"\n",
    "        return indxs\n",
    "    \n",
    "    def textify(self, indxs):\n",
    "        '''Textify observation codes (returns codes and descriptions)'''\n",
    "        txts = []\n",
    "        for i in indxs:\n",
    "            c,d,v,u,t = self.vocab_df.iloc[i]\n",
    "            if i == 0: txts.append((c, d))\n",
    "            else:      txts.append((f'{c}||{v}||{u}||{t}', d))\n",
    "        assert len(indxs) == len(txts), \"Possible bug, not all indxs being textified\"\n",
    "        return txts\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, obs_codes, num_buckets=5):\n",
    "        '''Create vocab object from observation codes'''\n",
    "        numerics = pd.DataFrame(obs_codes.loc[obs_codes['type'] == 'numeric',:])\n",
    "        texts = pd.DataFrame(obs_codes.loc[obs_codes['type'] == 'text',:])\n",
    "        numerics = numerics.astype({'value':'float'}, copy=False)\n",
    "        vocab_rows = []\n",
    "\n",
    "        for code in numerics.orig_code.unique():\n",
    "            this_code = numerics.loc[numerics['orig_code'] == code]\n",
    "            for unit in this_code.units.unique():\n",
    "                this_unit = this_code.loc[this_code['units'] == unit]\n",
    "                for val in np.linspace(this_unit.value.min(), this_unit.value.max(), num=num_buckets):\n",
    "                    vocab_rows.append([code,this_unit.desc.iloc[0],val,unit,'numeric'])\n",
    "\n",
    "        for code in texts.orig_code.unique():\n",
    "            this_code = texts.loc[texts['orig_code'] == code]\n",
    "            for unit in this_code.units.unique():\n",
    "                this_unit = this_code.loc[this_code['units'] == unit]\n",
    "                for val in this_unit.value.unique():\n",
    "                    vocab_rows.append([code,this_unit.desc.iloc[0],val,unit,'text'])\n",
    "\n",
    "        vocab_rows.insert(0, ['xxnone','Nothing recorded','xxnone','xxnone','xxnone'])\n",
    "        vocab_rows.insert(1, ['xxunk','Unknown','xxunk','xxunk','xxunk'])\n",
    "        obs_vocab = pd.DataFrame(data=vocab_rows, columns=['code','desc','value','units','type'])\n",
    "        assert obs_codes.orig_code.nunique() == obs_vocab.code.nunique()-2, \"Possible bug, obs_code nuniques don't match\"\n",
    "        return cls(obs_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ObsVocab.create\" class=\"doc_header\"><code>ObsVocab.create</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ObsVocab.create</code>(**`obs_codes`**, **`num_buckets`**=*`5`*)\n",
       "\n",
       "Create vocab object from observation codes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ObsVocab.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ObsVocab.numericalize\" class=\"doc_header\"><code>ObsVocab.numericalize</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ObsVocab.numericalize</code>(**`codes`**, **`verbose`**=*`True`*, **`log_dir`**=*`'./log'`*)\n",
       "\n",
       "Numericalize observation codes (return indices for codes)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ObsVocab.numericalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ObsVocab.textify\" class=\"doc_header\"><code>ObsVocab.textify</code><a href=\"__main__.py#L35\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ObsVocab.textify</code>(**`indxs`**)\n",
       "\n",
       "Textify observation codes (returns codes and descriptions)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ObsVocab.textify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`numericalize()` Explanation**\n",
    "- split incoming concated `code||value||units||type` string\n",
    "- get a result_df based on everything except value\n",
    "- then do an `argsort()` on the value column to determine closest value\n",
    " - based on example given in [pandas docs - cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html#building-criteria)\n",
    "   - **cookbook example that uses `loc` doesnt work, instead `iloc` [works](https://stackoverflow.com/questions/30112202/how-do-i-find-the-closest-values-in-a-pandas-series-to-an-input-number/53553226)**\n",
    " - `argsort()` - [Returns the indices that would sort this array](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort)\n",
    " - `[:1]` on that returns the one row with the closest match, index of that is what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note about logging numericalize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_code</th>\n",
       "      <th>desc</th>\n",
       "      <th>value</th>\n",
       "      <th>units</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8302-2</td>\n",
       "      <td>Body Height</td>\n",
       "      <td>48.8</td>\n",
       "      <td>cm</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72514-3</td>\n",
       "      <td>Pain severity - 0-10 verbal numeric rating [Sc...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>{score}</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29463-7</td>\n",
       "      <td>Body Weight</td>\n",
       "      <td>3.4</td>\n",
       "      <td>kg</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6690-2</td>\n",
       "      <td>Leukocytes [#/volume] in Blood by Automated count</td>\n",
       "      <td>5.2</td>\n",
       "      <td>10*3/uL</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>789-8</td>\n",
       "      <td>Erythrocytes [#/volume] in Blood by Automated ...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>10*6/uL</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     orig_code                                               desc value  \\\n",
       "indx                                                                      \n",
       "0       8302-2                                        Body Height  48.8   \n",
       "1      72514-3  Pain severity - 0-10 verbal numeric rating [Sc...   1.3   \n",
       "2      29463-7                                        Body Weight   3.4   \n",
       "3       6690-2  Leukocytes [#/volume] in Blood by Automated count   5.2   \n",
       "4        789-8  Erythrocytes [#/volume] in Blood by Automated ...   5.2   \n",
       "\n",
       "        units     type  \n",
       "indx                    \n",
       "0          cm  numeric  \n",
       "1     {score}  numeric  \n",
       "2          kg  numeric  \n",
       "3     10*3/uL  numeric  \n",
       "4     10*6/uL  numeric  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_vocab_obj = ObsVocab.create(obs_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 176, 16]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.numericalize(['8302-2||200.3||cm||numeric', \\\n",
    "                            '72514-3||4||{score}||numeric', '33756-8||21.7||mm||numeric','29463-7||181.8||kg||numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 9, 176, 16]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing unkown code\n",
    "obs_vocab_obj.numericalize(['blah-2||200.3||cm||numeric', \\\n",
    "                            '72514-3||4||{score}||numeric', '33756-8||21.7||mm||numeric','29463-7||181.8||kg||numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8302-2||158.45000000000002||cm||numeric', 'Body Height'),\n",
       " ('72514-3||2.475||{score}||numeric',\n",
       "  'Pain severity - 0-10 verbal numeric rating [Score] - Reported'),\n",
       " ('10834-0||3.35||g/dL||numeric', 'Globulin'),\n",
       " ('29463-7||122.44999999999999||kg||numeric', 'Body Weight')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.textify([5, 8, 200, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[545, 549, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.numericalize(['32465-7||Normal size prostate||{nominal}||text',\"80271-0||Positive Murphy's Sign||xxxnan||text\",\\\n",
    "                          'xxnone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('32465-7||Normal size prostate||{nominal}||text',\n",
       "  'Physical findings of Prostate'),\n",
       " (\"80271-0||Positive Murphy's Sign||xxxnan||text\",\n",
       "  'Physical findings of Abdomen by Palpation'),\n",
       " ('xxnone', 'Nothing recorded')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.textify([545, 549, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 497]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.numericalize(['xxnone','xxunk','72166-2||Never smoker||xxxnan||text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxnone', 'Nothing recorded'),\n",
       " ('xxunk||xxunk||xxunk||xxunk', 'Unknown'),\n",
       " ('8302-2||44.6||cm||numeric', 'Body Height'),\n",
       " ('8302-2||82.55000000000001||cm||numeric', 'Body Height'),\n",
       " ('72166-2||Never smoker||xxxnan||text', 'Tobacco smoking status NHIS')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab_obj.textify([0, 1, 2, 3, 497])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EhrVocabList:\n",
    "    '''Class to create and hold all vocab objects for an entire dataset'''\n",
    "    def __init__(self, demographics_vocabs, records_vocabs, age_mean, age_std, path):\n",
    "        self.demographics_vocabs, self.records_vocabs, self.path = demographics_vocabs, records_vocabs, path\n",
    "        self.age_mean, self.age_std = age_mean, age_std\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, path, num_buckets=5):\n",
    "        '''Read all code dfs from the dataset path and create all vocab objects'''\n",
    "        demographics_vocabs, records_vocabs = [], []\n",
    "        code_dfs = load_ehr_vocabcodes(path)\n",
    "        \n",
    "        def _get_demographics_codes(pt_codes):\n",
    "            code_dfs = []\n",
    "            code_dfs.extend([pd.DataFrame(range(1, 32, 1), columns=['code'])]) #31 days  \n",
    "            code_dfs.extend([pd.DataFrame(range(1, 13, 1), columns=['code'])]) #12 months \n",
    "            code_dfs.extend([pd.DataFrame(range(1900, pd.Timestamp.today().year + 1, 1), columns=['code'])]) #years 1900 to now\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.marital.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.race.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.ethnicity.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.gender.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.birthplace.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.city.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.state.dropna().unique(), columns=['code'])])\n",
    "            code_dfs.extend([pd.DataFrame(pt_codes.zip.dropna().unique(), columns=['code'])])\n",
    "            age_mean, age_std = pt_codes.age_now_days.mean(), pt_codes.age_now_days.std()\n",
    "            return code_dfs, age_mean, age_std\n",
    "        \n",
    "        demographics_codes, age_mean, age_std = _get_demographics_codes(code_dfs[0])\n",
    "        demographics_vocabs.extend([EhrVocab.create(codes_df) for codes_df in demographics_codes])\n",
    "        records_vocabs.extend([ObsVocab.create(code_dfs[1], num_buckets)])\n",
    "        records_vocabs.extend([EhrVocab.create(codes_df) for codes_df in code_dfs[2:]])\n",
    "        return cls(demographics_vocabs, records_vocabs, age_mean, age_std, path)    \n",
    "    \n",
    "    def save(self):\n",
    "        '''Save vocablist (containing all vocab objects for the dataset)'''\n",
    "        pckl_dir = Path(f'{self.path}/processed')\n",
    "        pckl_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pckl_f = open(f'{pckl_dir}/vocabs.vocablist', 'wb')\n",
    "        pickle.dump(self, pckl_f)\n",
    "        pckl_f.close()\n",
    "        print(f'Saved vocab lists to {pckl_dir}')\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        '''Load previously created vocablist object (containing all vocab objects for the dataset)'''\n",
    "        infile = open(f'{path}/processed/vocabs.vocablist','rb')\n",
    "        ehrVocabList = pickle.load(infile)\n",
    "        infile.close()\n",
    "        return ehrVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocabList.create\" class=\"doc_header\"><code>EhrVocabList.create</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocabList.create</code>(**`path`**, **`num_buckets`**=*`5`*)\n",
       "\n",
       "Read all code dfs from the dataset path and create all vocab objects"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocabList.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocabList.save\" class=\"doc_header\"><code>EhrVocabList.save</code><a href=\"__main__.py#L36\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocabList.save</code>()\n",
       "\n",
       "Save vocablist (containing all vocab objects for the dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocabList.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"EhrVocabList.load\" class=\"doc_header\"><code>EhrVocabList.load</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>EhrVocabList.load</code>(**`path`**)\n",
       "\n",
       "Load previously created vocablist object (containing all vocab objects for the dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EhrVocabList.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list_1K = EhrVocabList.create(PATH_1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocab lists to datasets/synthea/1K/processed\n"
     ]
    }
   ],
   "source": [
    "vocab_list_1K.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_1K = EhrVocabList.load(PATH_1K)\n",
    "obs_vocab, alg_vocab, crpl_vocab, med_vocab, img_vocab, proc_vocab, cnd_vocab, imm_vocab = vl_1K.records_vocabs\n",
    "bday, bmonth, byear, marital, race, ethnicity, gender, birthplace, city, state, zipcode  = vl_1K.demographics_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `records_vocabs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 36, 2]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_vocab.numericalize(['xxnone','65200003','428191000124101'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vocab.numericalize(['xxnone',344001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([36], [36])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_vocab.numericalize(['65200003']), proc_vocab.numericalize([65200003])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxnone', 'Nothing recorded'),\n",
       " ('xxunk', 'Unknown'),\n",
       " ('51185008', {'Chest', 'Thoracic structure (body structure)'}),\n",
       " ('12921003', {'Pelvis'}),\n",
       " ('40983000', {'Arm'}),\n",
       " ('8205005', {'Wrist'})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vocab.textify([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 6, 2, 3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vocab.numericalize(['xxnone','xxunk', 51299004,51185008,12921003]) #0,1,6,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxnone', 'Nothing recorded'),\n",
       " ('xxunk||xxunk||xxunk||xxunk', 'Unknown'),\n",
       " ('8302-2||44.6||cm||numeric', 'Body Height'),\n",
       " ('8302-2||82.55000000000001||cm||numeric', 'Body Height'),\n",
       " ('8302-2||120.5||cm||numeric', 'Body Height'),\n",
       " ('8302-2||158.45000000000002||cm||numeric', 'Body Height')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.textify([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10834-0||3.35||g/dL||numeric', 'Globulin')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.textify([200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 201, 16]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#expected 6, 9, 201, 16\n",
    "obs_vocab.numericalize(['8302-2||200.3||cm||numeric', \\\n",
    "                            '72514-3||4||{score}||numeric', '10834-0||3.7||g/dL||numeric','29463-7||181.8||kg||numeric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('786-4||35.25||g/dL||numeric', 'MCHC [Mass/volume] by Automated count'),\n",
       " ('2093-3||266.275||mg/dL||numeric', 'Total Cholesterol'),\n",
       " ('6206-7||71.1||kU/L||numeric', 'Peanut IgE Ab in Serum'),\n",
       " ('20505-4||1.175||mg/dL||numeric',\n",
       "  'Bilirubin.total [Mass/volume] in Urine by Test strip'),\n",
       " ('2075-0||107.6||mmol/L||numeric', 'Chloride'),\n",
       " ('46288-7||Surgical biopsy result abnormal||{nominal}||text',\n",
       "  'US Guidance for biopsy of Prostate')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_vocab.textify([50,150,250,350,450,548])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxnone', 'Nothing recorded'),\n",
       " ('xxunk', 'Unknown'),\n",
       " ('834061||START', {'Penicillin V Potassium 250 MG Oral Tablet'}),\n",
       " ('282464||START', {'Acetaminophen 160 MG Oral Tablet'}),\n",
       " ('313782||START', {'Acetaminophen 325 MG Oral Tablet'})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.textify([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxnone', 'xxunk', '834061||START', '282464||START', '313782||START']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.itoc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.numericalize(['xxnone', 'xxunk', '834061||START','282464||START', '313782||START', '749882||START']) #0,1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_vocab.numericalize(['834061||START'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `demographics_vocabs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 8)\n",
      "(14, 7)\n",
      "(124, 11)\n",
      "(5, 5)\n",
      "(7, 6)\n",
      "(25, 8)\n",
      "(4, 5)\n",
      "(205, 13)\n",
      "(211, 13)\n",
      "(3, 5)\n",
      "(200, 13)\n"
     ]
    }
   ],
   "source": [
    "for vocab in vl_1K.demographics_vocabs:\n",
    "    print(vocab.get_emb_dims())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 11, 32]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bday.numericalize(['xxnone','xxunk', 1,10,31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxnone', 'xxunk', '1', '10', '31']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bday.textify([0, 1, 2, 11, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmonth.textify([13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 49]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byear.numericalize(['1942',1947,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byear.numericalize([1948])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxnone', 'xxunk', 'M', 'S']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marital.textify([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxnone', 'xxunk', 'white', 'asian', 'black']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race.textify([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15885.602409638554, 9388.271666254166)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_1K.age_mean, vl_1K.age_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All Embedding Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_all_emb_dims(EhrVocabList, αd=0.5736):\n",
    "    '''Get embedding dimensions for all vocab objects of the dataset'''\n",
    "    demographics_dims = [vocab.get_emb_dims(αd) for vocab in EhrVocabList.demographics_vocabs]\n",
    "    recs_dims          = [vocab.get_emb_dims(αd) for vocab in EhrVocabList.records_vocabs]\n",
    "    \n",
    "#     emb_dims_list = [vocab.get_emb_dims() for vocab in vocabs_list]\n",
    "    demographics_dims_width = recs_dims_width = 0\n",
    "    for emb_dim in demographics_dims:\n",
    "        demographics_dims_width += emb_dim[1]\n",
    "    for emb_dim in recs_dims:\n",
    "        recs_dims_width += emb_dim[1]\n",
    "        \n",
    "    return demographics_dims, recs_dims, demographics_dims_width, recs_dims_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dims, recs_dims, demographics_dims_width, recs_dims_width = get_all_emb_dims(EhrVocabList.load(PATH_1K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 8),\n",
       " (14, 7),\n",
       " (124, 11),\n",
       " (5, 5),\n",
       " (7, 6),\n",
       " (25, 8),\n",
       " (4, 5),\n",
       " (205, 13),\n",
       " (211, 13),\n",
       " (3, 5),\n",
       " (200, 13)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(550, 17),\n",
       " (27, 8),\n",
       " (54, 9),\n",
       " (224, 13),\n",
       " (11, 6),\n",
       " (128, 12),\n",
       " (201, 13),\n",
       " (20, 7)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 85)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims_width, recs_dims_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_dims, recs_dims, demographics_dims_width, recs_dims_width = get_all_emb_dims(EhrVocabList.load(PATH_1K), αd=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 144),\n",
       " (14, 116),\n",
       " (124, 200),\n",
       " (5, 90),\n",
       " (7, 98),\n",
       " (25, 134),\n",
       " (4, 85),\n",
       " (205, 227),\n",
       " (211, 229),\n",
       " (3, 79),\n",
       " (200, 226)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(550, 291),\n",
       " (27, 137),\n",
       " (54, 163),\n",
       " (224, 232),\n",
       " (11, 109),\n",
       " (128, 202),\n",
       " (201, 226),\n",
       " (20, 127)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1628, 1487)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_dims_width, recs_dims_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_preprocessing_clean.ipynb.\n",
      "Converted 02_preprocessing_vocab.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
