{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing.clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean\n",
    "\n",
    "> Functions to split the raw EHR dataset, clean and save for further processing & vocab creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lemonpie.basics import *\n",
    "from lemonpie.preprocessing import clean\n",
    "from fastai.imports import *\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 13:43:38,393\tINFO services.py:1245 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.86.91',\n",
       " 'raylet_ip_address': '192.168.86.91',\n",
       " 'redis_address': '192.168.86.91:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-09-20_13-43-36_807789_9927/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-09-20_13-43-36_807789_9927/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2022-09-20_13-43-36_807789_9927',\n",
       " 'metrics_export_port': 59579,\n",
       " 'node_id': '76cb48652fa309ae956b87798a80ffb0fddcc57927b3d8b43eee85c7'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERENT_DATA_STORE = '/home/vinod/code/datasets/coherent'\n",
    "COHERENT_DATAGEN_DATE = '08-10-2021'\n",
    "COHERENT_CONDITIONS = {\n",
    "    \"heart_failure\" : \"88805009\",\n",
    "    \"coronary_heart\" : \"53741008\",\n",
    "    \"myocardial_infarction\" : \"22298006\",\n",
    "    \"stroke\" : \"230690007\",\n",
    "    \"cardiac_arrest\" : \"410429000\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherent Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retain only patients with FHIR bundles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_fhir_patients(coherent_path, csv_names):\n",
    "    \"\"\"Retain only patients with FHIR bundles.\"\"\"\n",
    "\n",
    "    # read pids with FHIR bundles\n",
    "    file_list = os.listdir(f'{coherent_path}/output/fhir')\n",
    "    fhir_pids = [((file).split(\"_\")[-1]).split(\".\")[0] for file in file_list]\n",
    "\n",
    "    # filter and retain only FHIR patients in all files\n",
    "    print(f\"Writing filtered files to {coherent_path}/raw_original/\")\n",
    "    for file in csv_names:\n",
    "        old_df = pd.read_csv(f\"{coherent_path}/output/csv/{file}.csv\", low_memory=False)\n",
    "        if file == 'patients':\n",
    "            fhir_mask = old_df.Id.isin(fhir_pids)\n",
    "        else:\n",
    "            fhir_mask = old_df.PATIENT.isin(fhir_pids)\n",
    "        new_df = old_df[fhir_mask]\n",
    "        assert len(new_df) == fhir_mask.sum(), f\"Count error in {file}\"\n",
    "        new_df.to_csv(f\"{coherent_path}/raw_original/{file}.csv\", index=False)\n",
    "        print(f\"Created {file} with {len(new_df)} records.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove ECG from observations and create ecg.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveout_ecg(coherent_path):\n",
    "    \"\"\"Move ECG data out of Observations into its own csv.\"\"\"\n",
    "    \n",
    "    old_obs = pd.read_csv(f\"{coherent_path}/raw_original/observations.csv\", low_memory=False)\n",
    "    ecg_obs = old_obs[old_obs[\"CODE\"] == \"29303009\"]\n",
    "    new_obs = old_obs.drop(ecg_obs.index)\n",
    "    assert len(new_obs) == len(old_obs) - len(ecg_obs), \"Mismatch after ECG removal from Observations\"\n",
    "    new_obs.to_csv(f\"{coherent_path}/raw_original/observations.csv\", index=False)\n",
    "    print(f\"Updated observations without ECG data = {len(new_obs)} records\")\n",
    "\n",
    "    ecg_obs.reset_index(inplace=True, drop=True)\n",
    "    odd_indxs = [i for i in range(1, len(ecg_obs), 2)]\n",
    "    ecg_obs.drop(odd_indxs, inplace=True)\n",
    "    ecg_obs.drop(columns=[\"ENCOUNTER\", \"CODE\", \"DESCRIPTION\", \"UNITS\", \"TYPE\"], inplace=True)\n",
    "    ecg_obs.rename(str.lower, axis='columns', inplace=True)\n",
    "    ecg_obs.to_csv(f\"{coherent_path}/ecg.csv\", index=False)\n",
    "    print(f\"Saved ECG data to {coherent_path}/ecg.csv with {len(ecg_obs)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create `modalities.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modalities_csv(coherent_path):\n",
    "    \"\"\"Create modalities csv.\"\"\"\n",
    "    \n",
    "    # dna - counts off by 1, because no FHIR bunde for 1 pt with dna data\n",
    "    dna_files = os.listdir(f'{coherent_path}/output/dna')\n",
    "    dna_pids = [file.split(\"_\")[-2] for file in dna_files]\n",
    "    \n",
    "    # mri\n",
    "    mri_files = os.listdir(f'{coherent_path}/output/dicom')\n",
    "    mri_pids = [file.split(\"_\")[-1].split(\".\")[0][:-1]  for file in mri_files]\n",
    "    \n",
    "    # ecg\n",
    "    ecg_data = pd.read_csv(f\"{coherent_path}/ecg.csv\")\n",
    "    ecg_pids = ecg_data.patient.unique()\n",
    "\n",
    "    # create modalities csv\n",
    "    patients = pd.read_csv(f\"{coherent_path}/raw_original/patients.csv\", low_memory=False)\n",
    "    modalities = patients[[\"Id\", \"FIRST\", \"LAST\"]].copy()\n",
    "    modalities.rename(str.lower, axis='columns', inplace=True)\n",
    "\n",
    "    modalities[\"mri\"] = patients.Id.isin(mri_pids)\n",
    "    modalities[\"dna\"] = patients.Id.isin(dna_pids)\n",
    "    modalities[\"ecg\"] = patients.Id.isin(ecg_pids)\n",
    "\n",
    "    modalities.to_csv(f\"{coherent_path}/modalities.csv\", index=False)\n",
    "    print(f\"Saved modalities to {coherent_path}/modalities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherent_preprocess(coherent_path=COHERENT_DATA_STORE, csv_names=FILENAMES):\n",
    "    \"\"\"Perform coherent-specific preprocessing.\"\"\"\n",
    "\n",
    "    # create raw_original dir\n",
    "    raw_dir = Path(f'{coherent_path}/raw_original')\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # filter patients to keep only those with FHIR bundles\n",
    "    print(\"--Filtering & retaining patients with FHIR bundles--\")\n",
    "    retain_fhir_patients(coherent_path, csv_names)\n",
    "\n",
    "    # move ECG data out of observations\n",
    "    print(\"--Moving ECG data out of observations into its own ecg.csv--\")\n",
    "    moveout_ecg(coherent_path)\n",
    "\n",
    "    # create modalities file\n",
    "    print(\"--Creating modalities.csv--\")\n",
    "    create_modalities_csv(coherent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Filtering & retaining patients with FHIR bundles--\n",
      "Writing filtered files to /home/vinod/code/datasets/coherent/raw_original/\n",
      "Created patients with 1278 records.\n",
      "Created observations with 705436 records.\n",
      "Created allergies with 106 records.\n",
      "Created careplans with 6135 records.\n",
      "Created medications with 209401 records.\n",
      "Created imaging_studies with 3752 records.\n",
      "Created procedures with 56092 records.\n",
      "Created conditions with 15956 records.\n",
      "Created immunizations with 11900 records.\n",
      "--Moving ECG data out of observations into its own ecg.csv--\n",
      "Updated observations without ECG data = 703292 records\n",
      "Saved ECG data to /home/vinod/code/datasets/coherent/ecg.csv with 1072 records\n",
      "--Creating modalities.csv--\n",
      "Saved modalities to /home/vinod/code/datasets/coherent/modalities.csv\n"
     ]
    }
   ],
   "source": [
    "coherent_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:: train: 0.6, valid: 0.2, test: 0.2\n",
      "Split patients into:: Train: 766, Valid: 256, Test: 256 -- Total before split: 1278\n",
      "Saved train data to /home/vinod/code/datasets/coherent/raw_split/train\n",
      "Saved valid data to /home/vinod/code/datasets/coherent/raw_split/valid\n",
      "Saved test data to /home/vinod/code/datasets/coherent/raw_split/test\n",
      "Completed - test\n",
      "Completed - valid\n",
      "\u001b[2m\u001b[36m(pid=11052)\u001b[0m Saved cleaned \"valid\" data to /home/vinod/code/datasets/coherent/cleaned/valid\n",
      "\u001b[2m\u001b[36m(pid=11055)\u001b[0m Saved cleaned \"test\" data to /home/vinod/code/datasets/coherent/cleaned/test\n",
      "\u001b[2m\u001b[36m(pid=11035)\u001b[0m Saved cleaned \"train\" data to /home/vinod/code/datasets/coherent/cleaned/train\n",
      "Completed - train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11035)\u001b[0m Saved vocab code tables to /home/vinod/code/datasets/coherent/cleaned/train/codes\n"
     ]
    }
   ],
   "source": [
    "clean.clean_raw_ehrdata(COHERENT_DATA_STORE, 0.2, 0.2, COHERENT_CONDITIONS, COHERENT_DATAGEN_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COHERENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs, valid_dfs, test_dfs = clean.load_cleaned_ehrdata(COHERENT_DATA_STORE)\n",
    "code_dfs = clean.load_ehr_vocabcodes(COHERENT_DATA_STORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in train_dfs:\n",
    "#     display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thispt = train_dfs[0].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient                      967d5226-f8c4-60a8-b882-6ef803af88a6\n",
       "birthdate                                              1930-04-29\n",
       "heart_failure                                               False\n",
       "heart_failure_age                                             NaN\n",
       "coronary_heart                                              False\n",
       "coronary_heart_age                                            NaN\n",
       "myocardial_infarction                                       False\n",
       "myocardial_infarction_age                                     NaN\n",
       "stroke                                                       True\n",
       "stroke_age                                                   87.0\n",
       "cardiac_arrest                                              False\n",
       "cardiac_arrest_age                                            NaN\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thispt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in code_dfs:\n",
    "#     display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure condition counts match - after extracting `y` for each patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`patients` dfs after cleaning, with `y` extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_train, pts_valid, pts_test = train_dfs[0], valid_dfs[0], test_dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conditions` dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnd_train, cnd_valid, cnd_test = train_dfs[8], valid_dfs[8], test_dfs[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests to ensure counts match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extract_ys(pt_dfs, cnd_dfs, conditions_dict=COHERENT_CONDITIONS):\n",
    "    \"\"\"Test for extract_ys function.\"\"\"\n",
    "    for pts_df, cnds_df, split in zip(pt_dfs, cnd_dfs, ['train','valid','test']):\n",
    "        print(f\"Checking {split} dfs...\")\n",
    "        for this_cnd in conditions_dict.keys():\n",
    "            code = f\"{conditions_dict[this_cnd]}||START\"\n",
    "            cnds_df_counts = len(cnds_df[cnds_df['code'] == code])\n",
    "            pts_df_counts = len(pts_df[pts_df[this_cnd] == 1])\n",
    "            assert cnds_df_counts == pts_df_counts, f\"Error in {split} for {this_cnd} -- {cnds_df_counts} != {pts_df_counts}\"\n",
    "\n",
    "        print(f\"Tests passed for {split} - all condition counts match\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking train dfs...\n",
      "Tests passed for train - all condition counts match\n",
      "Checking valid dfs...\n",
      "Tests passed for valid - all condition counts match\n",
      "Checking test dfs...\n",
      "Tests passed for test - all condition counts match\n"
     ]
    }
   ],
   "source": [
    "test_extract_ys([pts_train, pts_valid, pts_test],[cnd_train, cnd_valid, cnd_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'heart_failure': 189,\n",
       "  'coronary_heart': 194,\n",
       "  'myocardial_infarction': 82,\n",
       "  'stroke': 435,\n",
       "  'cardiac_arrest': 107},\n",
       " {'heart_failure': 68,\n",
       "  'coronary_heart': 66,\n",
       "  'myocardial_infarction': 28,\n",
       "  'stroke': 138,\n",
       "  'cardiac_arrest': 30},\n",
       " {'heart_failure': 75,\n",
       "  'coronary_heart': 76,\n",
       "  'myocardial_infarction': 35,\n",
       "  'stroke': 125,\n",
       "  'cardiac_arrest': 43}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.get_label_counts([pts_train, pts_valid, pts_test], conditions_dict=COHERENT_CONDITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf702de-f751-45dc-93c8-61dc5e6ad297",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['heart_failure', 'coronary_heart', 'myocardial_infarction', 'stroke', 'cardiac_arrest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lemonpie.preprocessing import vocab, transform\n",
    "from lemonpie.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since data is pre-cleaned, skipping Cleaning, Splitting and Vocab-creation\n",
      "------------------- Creating patient lists -------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/vinod/code/datasets/coherent/processed/vocabs.vocablist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-d1e7d4967670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform.preprocess_ehr_dataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mCOHERENT_DATA_STORE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mCOHERENT_DATAGEN_DATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconditions_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOHERENT_CONDITIONS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mage_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/myrepos/lemonpie/lemonpie/preprocessing/transform.py\u001b[0m in \u001b[0;36mpreprocess_ehr_dataset\u001b[0;34m(path, today, conditions_dict, valid_pct, test_pct, obs_vocab_buckets, age_start, age_stop, age_in_months, vocab_path, from_raw_data)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------------- Creating patient lists -------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mcreate_all_ptlists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_in_months\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/myrepos/lemonpie/lemonpie/preprocessing/transform.py\u001b[0m in \u001b[0;36mcreate_all_ptlists\u001b[0;34m(path, age_start, age_stop, age_in_months, vocab_path, verbose, delete_existing)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mall_dfs_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cleaned_ehrdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#train_dfs, valid_dfs, test_dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m     \u001b[0mvocablist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEhrVocabList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mall_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_dfs_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/myrepos/lemonpie/lemonpie/preprocessing/vocab.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;34m'''Load previously created vocablist object (containing all vocab objects for the dataset)'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/processed/vocabs.vocablist'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mehrVocabList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/vinod/code/datasets/coherent/processed/vocabs.vocablist'"
     ]
    }
   ],
   "source": [
    "transform.preprocess_ehr_dataset(\n",
    "    COHERENT_DATA_STORE, \n",
    "    COHERENT_DATAGEN_DATE, \n",
    "    conditions_dict=COHERENT_CONDITIONS, \n",
    "    age_start=240, \n",
    "    age_stop=360, \n",
    "    age_in_months=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ae1be-20ce-4e57-99b3-44dada24b413",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/vinod/code/datasets/coherent/processed/vocabs.vocablist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-86b4cd2dc38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcoherent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEHRData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOHERENT_DATA_STORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m360\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_in_months\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy_load_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdemograph_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemograph_dims_wd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_dims_wd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_emb_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEhrVocabList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOHERENT_DATA_STORE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pos_wts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pos_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoherent_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/myrepos/lemonpie/lemonpie/preprocessing/vocab.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;34m'''Load previously created vocablist object (containing all vocab objects for the dataset)'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/processed/vocabs.vocablist'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mehrVocabList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/vinod/code/datasets/coherent/processed/vocabs.vocablist'"
     ]
    }
   ],
   "source": [
    "coherent_data = EHRData(COHERENT_DATA_STORE, labels, age_start=240, age_stop=360, age_in_months=True, lazy_load_gpu=False)\n",
    "demograph_dims, rec_dims, demograph_dims_wd, rec_dims_wd = vocab.get_all_emb_dims(transform.EhrVocabList.load(COHERENT_DATA_STORE))\n",
    "train_dl, valid_dl, train_pos_wts, valid_pos_wts = coherent_data.get_data(bs=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b954a",
   "metadata": {},
   "source": [
    "#### `EHR_LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59926c-f0e1-4b29-9fe8-b4cd484fc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_LSTM(demograph_dims, rec_dims, demograph_dims_wd, rec_dims_wd, num_labels=len(labels)).to(DEVICE)\n",
    "train_loss_fn, valid_loss_fn = get_loss_fn(train_pos_wts), get_loss_fn(valid_pos_wts)\n",
    "optimizer = torch.optim.Adagrad(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88739f-f5e9-421f-a43c-e6181a5af592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(train_dl), len(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557fd29-491f-438d-b72e-778ba513901e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EHR_LSTM(\n",
       "  (embs): ModuleList(\n",
       "    (0): Embedding(40, 8)\n",
       "    (1): Embedding(16, 8)\n",
       "    (2): Embedding(128, 8)\n",
       "    (3): Embedding(8, 8)\n",
       "    (4): Embedding(8, 8)\n",
       "    (5): Embedding(8, 8)\n",
       "    (6): Embedding(8, 8)\n",
       "    (7): Embedding(248, 16)\n",
       "    (8): Embedding(208, 16)\n",
       "    (9): Embedding(8, 8)\n",
       "    (10): Embedding(184, 16)\n",
       "  )\n",
       "  (embgs): ModuleList(\n",
       "    (0): EmbeddingBag(536, 16, mode=mean)\n",
       "    (1): EmbeddingBag(32, 8, mode=mean)\n",
       "    (2): EmbeddingBag(56, 8, mode=mean)\n",
       "    (3): EmbeddingBag(232, 16, mode=mean)\n",
       "    (4): EmbeddingBag(16, 8, mode=mean)\n",
       "    (5): EmbeddingBag(144, 8, mode=mean)\n",
       "    (6): EmbeddingBag(184, 16, mode=mean)\n",
       "    (7): EmbeddingBag(24, 8, mode=mean)\n",
       "  )\n",
       "  (input_dp): InputDropout()\n",
       "  (lstm): LSTM(88, 88, num_layers=4, batch_first=True, dropout=0.3)\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=208, out_features=416, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=416, out_features=832, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=832, out_features=1664, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (lin_o): Linear(in_features=3328, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ae35a-4048-44c0-b3f4-a80a76f9e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_1K = RunHistory(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561167cc",
   "metadata": {},
   "source": [
    "`use_amp=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('lemonpie')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
