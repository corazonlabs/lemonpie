# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_data.ipynb (unless otherwise specified).

__all__ = ['EHRDataSplits', 'LabelEHRData', 'EHRDataset', 'MultimodalBatchSampler', 'create_multimodal_ds_sampler',
           'EHRData']

# Cell
from .basics import *
from .preprocessing.transform import *
from fastai.imports import *
import copy

# Cell
class EHRDataSplits():
    '''Class to hold the PatientList splits'''
    def __init__(self, path, age_start, age_range, start_is_date, age_in_months):
        self.train, self.valid, self.test = self._load_splits(path, age_start, age_range, start_is_date, age_in_months)

    def _load_splits(self, path, age_start, age_range, start_is_date, age_in_months):
        '''Load splits of preprocessed `PatientList`s from persistent store using path'''
        train = PatientList.load(path, 'train', age_start, age_range, start_is_date, age_in_months)
        valid = PatientList.load(path, 'valid', age_start, age_range, start_is_date, age_in_months)
        test  = PatientList.load(path, 'test',  age_start, age_range, start_is_date, age_in_months)
        return train, valid, test

    def get_splits(self):
        '''Return splits'''
        return self.train, self.valid, self.test

    def get_lengths(self):
        '''Return a dataframe with lengths (# of patients) of the splits (train, valid, test) and total'''
        lengths = [len(self.train), len(self.valid), len(self.test), len(self.train)+len(self.valid)+len(self.test)]
        return pd.DataFrame(lengths, index=['train','valid','test','total'], columns=['lengths'])

    def get_label_counts(self, labels):
        '''Get prevalence counts of labels in each split - returns a dataframe with counts for each split and total count'''
        counts = []
        for label in labels:
            train_count = [self.train[i].conditions[label] == 1 for i in range(len(self.train))].count(True)
            valid_count = [self.valid[i].conditions[label] == 1 for i in range(len(self.valid))].count(True)
            test_count  = [self.test[i].conditions[label] == 1 for i in range(len(self.test))].count(True)
            total_count = train_count+valid_count+test_count
            counts.append([train_count, valid_count, test_count, total_count])
        return pd.DataFrame(counts, index=labels, columns=['train','valid','test','total'])

    def get_pos_wts(self, labels):
        '''Get positive weights to be used in `nn.BCEWithLogitsLoss`'''
        pos_counts = self.get_label_counts(labels)
        neg_counts = self.get_lengths().transpose().values - pos_counts
        return round(neg_counts / pos_counts)

# Cell
class LabelEHRData():
    '''Class to hold labeled EHR data splits'''
    def __init__(self, train, valid, test, labels):
        '''Extracts y from patient object, each labelset a tuple of x,y: x=Patient object, y=tensor of conditions'''
        self.x_train, self.y_train = train, self._get_y(train, labels)
        self.x_valid, self.y_valid = valid, self._get_y(valid, labels)
        self.x_test,  self.y_test  = test , self._get_y(test , labels)

        self.train = self.x_train, self.y_train
        self.valid = self.x_valid, self.y_valid
        self.test  = self.x_test,  self.y_test

    def _get_y(self, ds, labels):
        '''Extract y from each patient object in ds and stack them - ds is dataset containing patient objects'''
        y = []
        for pt in ds:
            y.append( torch.FloatTensor(np.array([pt.conditions[label] for label in labels], dtype='float')) )
        return torch.stack(y)

# Cell
class EHRDataset(torch.utils.data.Dataset):
    """Class to hold a single EHR dataset (holds a tuple of x, y & m for modality type).
    Also handles lazy vs full loading of dataset on GPU."""

    def __init__(
        self,
        x_labeled: list,
        y_labeled: Tensor,
        modality_type: int,
        lazy_load_gpu: bool = True,
    ):
        """If `lazy_load_gpu` is `False`, load entire dataset on GPU."""
        self.m = torch.full((len(x_labeled), 1), modality_type)
        if lazy_load_gpu:
            self.x, self.y = x_labeled, y_labeled
            self.lazy = True
        else:
            self.x = [x.to_gpu() for x in x_labeled]
            self.y = y_labeled.to(DEVICE)
            self.m = self.m.to(DEVICE)
            self.lazy = False

    def __len__(self):
        return len(self.x)

    def _test_getitem(self, i):
        return self.x[i], self.y[i], self.m[i]

    def __getitem__(self, i):
        """If lazy loading, return deep copy of patient object `i`
        else entire dataset already on GPU - just return `i`"""
        if self.lazy:
            return copy.deepcopy(self.x[i]), self.y[i], self.m[i]
        else:
            return self.x[i], self.y[i], self.m[i]


# Cell
class MultimodalBatchSampler(Sampler):
    """Custom BatchSampler for multimodal data."""

    def __init__(self, indices_list: list, batch_size: int, shuffle: bool):
        """Init with indicies from every modality-type dataset."""
        self.indices_list = indices_list
        self.batch_size = batch_size
        self.shuffle = shuffle

    def _chunk(self, indices, size):
        """Chunk indices into batch size."""
        return torch.split(torch.tensor(indices), size)

    def __iter__(self):
        """Iterable used by dataloaders."""
        all_batches = []
        for indices in self.indices_list:
            if self.shuffle:
                random.shuffle(indices)
            all_batches.extend(self._chunk(indices, self.batch_size))
        all_batches = [batch.tolist() for batch in all_batches]
        if self.shuffle:
            random.shuffle(all_batches)

        return iter(all_batches)

    def __len__(self):
        """Return length based on concated datasets."""
        return sum(len(indices) for indices in self.indices_list) // self.batch_size


# Cell
def create_multimodal_ds_sampler(
    ehr_dataset_list: list, batch_size: int, shuffle: bool
):
    """Create a custom ConcatDataset and BatchSampler for multimodal data."""

    mm_dataset = torch.utils.data.ConcatDataset(ehr_dataset_list)
    indxs = mm_dataset.cumulative_sizes

    indicies_list = []
    for i in range(len(ehr_dataset_list)):
        if i == 0:
            indx_range = range(indxs[0])
        else:
            indx_range = range(indxs[i - 1], indxs[i])
        indicies_list.append(list(indx_range))

    batch_sampler = MultimodalBatchSampler(indicies_list, batch_size, shuffle)

    return mm_dataset, batch_sampler


# Cell
class EHRData:
    """All encompassing class for EHR data
    Holds Splits, Labels, Datasets, DataLoaders and
    provides convenience fns for training and prediction."""

    def __init__(
        self,
        path,
        labels,
        age_start,
        age_range,
        start_is_date,
        age_in_months,
        lazy_load_gpu=True,
    ):
        self.path, self.labels = path, labels
        self.age_start, self.age_range = age_start, age_range
        self.start_is_date, self.age_in_months = start_is_date, age_in_months
        self.lazy_load_gpu = lazy_load_gpu

    def load_splits(self, modality_type):
        """Load data splits given dataset path"""
        self.splits = EHRDataSplits(
            self.path,
            modality_type,
            self.age_start,
            self.age_range,
            self.start_is_date,
            self.age_in_months,
        )

    def label(self):
        """Run labeler - i.e. extract y from patient objects"""
        self.labeled = LabelEHRData(*self.splits.get_splits(), self.labels)

    def create_datasets(self, modality_type):
        """Create `EHRDataset`s"""
        self.train_ds = EHRDataset(*self.labeled.train, modality_type, self.lazy_load_gpu)
        self.valid_ds = EHRDataset(*self.labeled.valid, modality_type, self.lazy_load_gpu)
        self.test_ds = EHRDataset(*self.labeled.test, modality_type, self.lazy_load_gpu)

    def ehr_collate(b):
        """Custom collate function for use in `DataLoader`"""
        xs,ys, ms = zip(*b)
        return xs, torch.stack(ys), torch.stack(ms)

    def create_dls(self, bs, lazy, c_fn=ehr_collate, **kwargs):
        """Create `DataLoader`s"""
        self.train_dl = DataLoader(
            self.train_ds, bs, shuffle=True, collate_fn=c_fn, pin_memory=lazy, **kwargs
        )
        self.valid_dl = DataLoader(
            self.valid_ds, bs * 2, collate_fn=c_fn, pin_memory=lazy, **kwargs
        )
        self.test_dl = DataLoader(
            self.test_ds, bs * 2, collate_fn=c_fn, pin_memory=lazy, **kwargs
        )

    def _per_modality(self, modality_type, bs=64, num_workers=0):
        """Return all data per modality."""
        self.load_splits(modality_type)
        self.label()
        self.create_datasets(modality_type)
        self.create_dls(bs, self.lazy_load_gpu, num_workers=num_workers)

        pos_wts_df = self.splits.get_pos_wts(self.labels)
        pos_wts = {}
        pos_wts["train"] = torch.Tensor(pos_wts["train"].values)
        pos_wts["valid"] = torch.Tensor(pos_wts["valid"].values)
        pos_wts["test"] = torch.Tensor(pos_wts["test"].values)
        return self.train_dl, self.valid_dl, self.test_dl, pos_wts

    def get_data(self, bs=64, num_workers=0):
        """Return all data for every modality."""
        modality_types = os.listdir(f"{self.path}/processed")
        data = {}
        for m in modality_types:
            data[m] = self._per_modality(m, bs, num_workers)

        return data
