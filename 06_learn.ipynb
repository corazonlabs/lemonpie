{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learn\n",
    "> Classes and functions for training and predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lemonade.setup import * \n",
    "from lemonade.metrics import * \n",
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `save_to_checkpoint()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_to_checkpoint(epoch_index, model, optimizer, path):\n",
    "    '''Save model and optimizer state_dicts to checkpoint'''\n",
    "    if not os.path.isdir(path): Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch_index':epoch_index,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, f'{path}/checkpoint.tar')\n",
    "    print(f'Checkpointed to \"{path}/checkpoint.tar\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_from_checkpoint()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_from_checkpoint(model, path, optimizer=None, for_inference=False):\n",
    "    '''Load from checkpoint - model, optimizer & epoch_index for training or just model for inference'''\n",
    "    \n",
    "    print(f'From \"{path}/checkpoint.tar\", loading model ...')\n",
    "    chkpt = torch.load(f'{path}/checkpoint.tar')\n",
    "    model.load_state_dict(chkpt['model_state_dict'])\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    if for_inference:\n",
    "        return model\n",
    "    else:\n",
    "        print(f'loading optimizer and epoch_index ...')\n",
    "        optimizer.load_state_dict(chkpt['optimizer_state_dict'])\n",
    "        return chkpt['epoch_index'], model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_loss_fn()` - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_loss_fn(pos_wts):\n",
    "    '''Return `nn.BCEWithLogitsLoss` with the given positive weights'''\n",
    "    return nn.BCEWithLogitsLoss(pos_weight=pos_wts).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class `RunHistory` - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class RunHistory:\n",
    "    '''Class to hold training and prediction run histories'''\n",
    "    def __init__(self, labels):\n",
    "        self.train = self.valid = self.test = pd.DataFrame(columns=['loss', *labels])\n",
    "        self.y_train = self.yhat_train = self.y_valid = self.yhat_valid = self.y_test = self.yhat_test = []\n",
    "        self.prediction_summary = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- `y` and `y_hat` are actual (ground truth and predicted) values from \n",
    "    - the last epoch of `fit()` for `train` and `valid`, \n",
    "    - the last run of `predict()` for `test` \n",
    "- `train`, `valid` and `test` are calculated loss and accuracy values at the end of each epoch\n",
    "    - for `test` there is only a single epoch in each run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit & predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BCEWithLogitsLoss` & `torch.sigmoid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `BCEWithLogitsLoss` because its [more numerically stable than using a plain Sigmoid followed by a BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
    "- Also accomodates multi-label classification & class-imbalanced datasets due to the use of `pos_weights`\n",
    "- But this means that the model does not do a final sigmoid at the output layer, since thats done by the loss function\n",
    "- So need to do a `torch.sigmoid` on the `yhat`s before using them to calculate accuracy\n",
    "- 2 good discussions on this topic\n",
    "    - [How to interpret the probability of classes in binary classification?](https://discuss.pytorch.org/t/how-to-interpret-the-probability-of-classes-in-binary-classification/45709)\n",
    "    - [BCEWithLogitsLoss and model accuracy calculation](https://discuss.pytorch.org/t/bcewithlogitsloss-and-model-accuracy-calculation/59293/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(model, train_dl, train_loss_fn, optimizer, lazy=True):\n",
    "    '''Train model using train dataset'''\n",
    "    yhat_train = y_train = Tensor([])\n",
    "    train_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "        if lazy: xb, yb = [x.to_gpu(non_block=True) for x in xb], yb.to(DEVICE, non_blocking=True)\n",
    "        y_hat  = model(xb)\n",
    "        loss   = train_loss_fn(y_hat, yb)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        yhat_train = torch.cat((yhat_train, torch.sigmoid( y_hat.cpu().detach() ) ))\n",
    "        y_train    = torch.cat((y_train, yb.cpu().detach()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        \n",
    "    return train_loss, yhat_train, y_train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `evaluate()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate(model, eval_dl, eval_loss_fn, lazy=True):\n",
    "    '''Evaluate model - used for validation (while training) and prediction'''\n",
    "    yhat_eval = y_eval = Tensor([])\n",
    "    eval_loss = 0.\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():                                  \n",
    "        for xb, yb in eval_dl:  \n",
    "            if lazy: xb, yb = [x.to_gpu(non_block=True) for x in xb], yb.to(DEVICE, non_blocking=True)\n",
    "            y_hat  = model(xb)                             \n",
    "\n",
    "            eval_loss += (eval_loss_fn(y_hat, yb)).item()\n",
    "            yhat_eval = torch.cat((yhat_eval, torch.sigmoid( y_hat.cpu().detach() ) ))\n",
    "            y_eval    = torch.cat((y_eval, yb.cpu().detach()))    \n",
    "        \n",
    "    return eval_loss, yhat_eval, y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fit(epochs, history, model, train_loss_fn, valid_loss_fn, optimizer, accuracy_fn, \n",
    "        train_dl, valid_dl, lazy=True, to_chkpt_path=None, from_chkpt_path=None, verbosity=0.75):\n",
    "    '''Fit model and return results in `history`'''\n",
    "    \n",
    "    if from_chkpt_path: \n",
    "        last_epoch, model, optimizer = load_from_checkpoint(model, from_chkpt_path, optimizer, for_inference=False)\n",
    "        start_epoch = last_epoch+1\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "    end_epoch = start_epoch+(epochs-1)\n",
    "    print_epochs = np.linspace(start_epoch, end_epoch, int(epochs*verbosity), endpoint=True, dtype=int)\n",
    "    train_history, valid_history = [], []\n",
    "    \n",
    "    print('{:>5} {:>16} {:^20} {:>25} {:^20}'.format('epoch |', 'train loss |', 'train aurocs', 'valid loss |', 'valid aurocs'))\n",
    "    print('{:-^100}'.format('-'))\n",
    "    \n",
    "    for epoch in range (start_epoch, start_epoch+epochs):\n",
    "        \n",
    "        train_loss, yhat_train, y_train, model = train(model, train_dl, train_loss_fn, optimizer, lazy)\n",
    "        valid_loss, yhat_valid, y_valid = evaluate(model, valid_dl, valid_loss_fn, lazy)                \n",
    "        \n",
    "        train_loss,   valid_loss   = train_loss/len(train_dl), valid_loss/len(valid_dl)\n",
    "        train_aurocs, valid_aurocs = accuracy_fn(y_train, yhat_train), accuracy_fn(y_valid, yhat_valid)\n",
    "        train_history.append([train_loss, *train_aurocs])\n",
    "        valid_history.append([valid_loss, *valid_aurocs])\n",
    "        \n",
    "        if epoch in print_epochs:\n",
    "            row  = f'{epoch:>5} |'\n",
    "            row += f'{train_loss:>15.3f} | '\n",
    "            row += f'{[f\"{a:.3f}\" for a in train_aurocs[:4]]}'\n",
    "            row += f'{valid_loss:>19.3f} | '\n",
    "            row += f'{[f\"{a:.3f}\" for a in valid_aurocs[:4]]}'\n",
    "            print(re.sub(\"',*\", \"\", row))\n",
    "            \n",
    "    if to_chkpt_path: save_to_checkpoint(end_epoch, model, optimizer, to_chkpt_path)\n",
    "\n",
    "    h = history\n",
    "    h.y_train, h.yhat_train, h.y_valid, h.yhat_valid = y_train, yhat_train, y_valid, yhat_valid\n",
    "    h.train = h.train.append(pd.DataFrame(train_history, columns=h.train.columns), ignore_index=True)\n",
    "    h.valid = h.valid.append(pd.DataFrame(valid_history, columns=h.valid.columns), ignore_index=True)\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `predict()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def predict(history, model, test_loss_fn, accuracy_fn, test_dl, chkpt_path, lazy=True):\n",
    "    '''Predict and return results in `history`'''\n",
    "    model = load_from_checkpoint(model, chkpt_path, for_inference=True)\n",
    "    test_loss, yhat_test, y_test = evaluate(model, test_dl, test_loss_fn, lazy) \n",
    "            \n",
    "    test_loss = test_loss/len(test_dl)\n",
    "    test_aurocs = accuracy_fn(y_test, yhat_test)\n",
    "    print(f'test loss = {test_loss}')\n",
    "    print(f'test aurocs = {test_aurocs}')\n",
    "    \n",
    "    h = history\n",
    "    h.test = pd.DataFrame([[test_loss, *test_aurocs]], columns=h.test.columns)\n",
    "    h.y_test, h.yhat_test = y_test, yhat_test\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_loss()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_loss(history_df, title='Loss', axis=None):\n",
    "    '''Plot loss'''\n",
    "    if axis == None:\n",
    "        fig = plt.figure(figsize=(8,5))\n",
    "        axis = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    axis.plot(range(len(history_df)), history_df['loss'], label=title)\n",
    "\n",
    "    axis.set_title(title)\n",
    "    axis.set_xlabel('epochs')\n",
    "    axis.set_ylabel('loss')\n",
    "    axis.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_losses()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_losses(train_history, valid_history):\n",
    "    '''Plot multiple losses (train and valid) side by side'''\n",
    "    fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_loss(train_history, title='Train Loss', axis=axes[0])\n",
    "    plot_loss(valid_history, title='Valid Loss', axis=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_aurocs()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_aurocs(history_df, title='AUROC Scores', axis=None):\n",
    "    '''Plot AUROC scores'''\n",
    "    if axis == None:\n",
    "        fig = plt.figure(figsize=(8,5))\n",
    "        axis = fig.add_axes([0,0,1,1])\n",
    "    for lbl in history_df.columns[1:]:\n",
    "        axis.plot(range(len(history_df)), history_df[lbl], label=f'{lbl} (final: {history_df[lbl].iat[-1]:.3f})')\n",
    "    axis.set_title(title)\n",
    "    axis.set_xlabel('epochs')\n",
    "    axis.set_ylabel('auroc scores')\n",
    "    axis.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_train_valid_aurocs()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_train_valid_aurocs(train_history, valid_history):\n",
    "    '''Plot train and valid AUROC scores side by side'''\n",
    "    fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "    plt.tight_layout()\n",
    "    plot_aurocs(train_history, title='Train - AUROC Scores', axis=axes[0])\n",
    "    plot_aurocs(valid_history, title='Valid - AUROC Scores', axis=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_fit_results()` - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_fit_results(history, labels):\n",
    "    '''All plots after fit - ROC curves, losses and AUROCs'''\n",
    "    h = history\n",
    "    train_rocs, valid_rocs = MultiLabelROC(h.y_train, h.yhat_train, labels), MultiLabelROC(h.y_valid, h.yhat_valid, labels)\n",
    "    plot_train_valid_rocs(train_rocs.ROCs, valid_rocs.ROCs, labels, multilabel=True)\n",
    "    plot_losses(h.train, h.valid)\n",
    "    plot_train_valid_aurocs(h.train, h.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `summarize_prediction()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summarize_prediction(history, labels, plot=True):\n",
    "    '''Summarize after prediction - plot ROC curves, calculate auroc, optimal threshold and 95% CI for AUROC and return results in `history`'''\n",
    "    h = history\n",
    "    test_rocs = MultiLabelROC(h.y_test, h.yhat_test, labels)\n",
    "    if plot: plot_rocs(test_rocs.ROCs, labels, title='Test ROC curves', multilabel=True)\n",
    "    print('\\nPrediction Summary ...')    \n",
    "    col_names = ['auroc_score', 'optimal_threshold', 'auroc_95_ci']\n",
    "    rows = []\n",
    "    for i, label in enumerate(labels):\n",
    "        row = [test_rocs.ROCs[label].auroc, test_rocs.ROCs[label].optimal_thresh(), auroc_ci(h.y_test[:,i], h.yhat_test[:,i])]\n",
    "        rows.append(row)    \n",
    "    history.prediction_summary = pd.DataFrame(rows, index=labels, columns=col_names)        \n",
    "    print(history.prediction_summary)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `count_parameters()` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def count_parameters(model, printout=False):\n",
    "    '''Returns number of parameters in model'''\n",
    "    total         = sum(p.numel() for p in model.parameters())\n",
    "    trainable     = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad==False)\n",
    "    assert total == (trainable+non_trainable)\n",
    "    print(f'total: {total:,}, trainable: {trainable:,}, non_trainable: {non_trainable:,}')\n",
    "    return total, trainable, non_trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_setup.ipynb.\n",
      "Converted 01_preprocessing_clean.ipynb.\n",
      "Converted 02_preprocessing_vocab.ipynb.\n",
      "Converted 03_preprocessing_transform.ipynb.\n",
      "Converted 04_data.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_learn.ipynb.\n",
      "Converted 07_models.ipynb.\n",
      "Converted 08_experiment.ipynb.\n",
      "Converted 99_quick_walkthru.ipynb.\n",
      "Converted 99_running_exps.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
